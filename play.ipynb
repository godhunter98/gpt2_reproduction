{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297698f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harsh/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "805e3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40000  202651 1115394 input.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(45929) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!wc input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c048a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "for k,v in sd_hf.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9148551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1101, -0.0393,  0.0331,  0.1338, -0.0485, -0.0789, -0.2398, -0.0895,\n",
       "         0.0253, -0.1074, -0.1811, -0.0672,  0.0739, -0.0161,  0.0117,  0.1245,\n",
       "        -0.0020, -0.0815,  0.0338,  0.2365])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_hf['transformer.wte.weight'].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c80c448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model_hf.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154a1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "millnames = ['',' Thousand',' Million',' Billion',' Trillion']\n",
    "\n",
    "def millify(n):\n",
    "    n = float(n)\n",
    "    millidx = max(0,min(len(millnames)-1,\n",
    "                        int(math.floor(0 if n == 0 else math.log10(abs(n))/3))))\n",
    "\n",
    "    return '{:.0f}{}'.format(n / 10**(3 * millidx), millnames[millidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6f6794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of params in the model is 124 Million!\n"
     ]
    }
   ],
   "source": [
    "print(f'The total number of params in the model is {millify(num_params)}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1762307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, and my project is based on the idea of a language model.\\n\\nI want to have a language that's both expressive and readable.\\n\\nLet's look at the following code:\\n\\nimport Data.ByteString\\n\\ndef get ( self ):\\n\\nself.data = Data.ByteString(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data.get())\\n\\nself.data.append(data\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a language model, and if I don't understand it, I won't learn to write it. So I was writing a system for programming in JavaScript and a system for writing systems for programming in Python and a system for writing systems for programming in C++. And then I realized I was writing a language model, not a language model, because I was writing a language model for programming in JavaScript and a language model for writing systems for programming in Python and a language model for writing systems for programming in C++. And I realized that I needed something to write things that were a bit more complicated. And so I wrote a system for programming in JavaScript and a system for writing systems for programming in Python and a system for writing systems for programming in C++. And I realized that I needed something to write things that were a bit more complicated. And so I decided to give up on the language model. And so I decided to give up on the language model.\\n\\nAnd so I took a look at the documentation and realized something was wrong. I realized that the documentation was about building systems that were not a bit more complicated. And so I decided to give up on the language model. And so I took a look at the documentation and realized something was wrong. I\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'm trying to learn some things. I'm trying to learn a lot of things without being able to understand what's going on.\\n\\nI think this is a common theme with people in the field and in the software industry. There are always people who want to talk about how the world is different, but at the same time they are still trying to learn things. The question is, can you create something that's really easy for people to understand and that's more fun, more accessible, in a way?\\n\\nJ: Yeah, but I think there's some things that we've been really good at. The thing that's really important is to make it accessible for everybody, and because we've been doing it for a while, so we've learned a lot. We've also learned a lot of other things. We've been really good at creating great things, and I think that's one of the things that is really important.\\n\\nM: What's your favorite project, and what's it been like to make it?\\n\\nJ: I'm very happy with it. It's a very difficult project. It's very hard. It's very hard to create something that is fun and accessible and interesting. I think we all like to\"},\n",
       " {'generated_text': \"Hello, I'm a language model, but I don't believe in a language model. I'm a language model. I don't like to think of myself as a model, but I do want to do better than I do right now. So I think that the future is not going to be built on a theory that I'm not familiar with. The future is being built on a theory that I'm not familiar with.\\n\\nI'm also a language model. I'm a language model. Even though I'm very vocal about the need for strong, cohesive, deep software, I think it's important to understand both the language model and the software model as well. I think the best way to think about it is by going back to a very early age where you can't see the human language. You can't see the computer language. You can't see the human brain. You can't see the computer brain. You can't see the human mind. You can't see the human mind.\\n\\nSo it's not like you can't see the human mind. It's like you can't see the human mind. It's like you can't see the human brain. It's like you can't see the human mind.\\n\\nSo it's not like you can't see the human\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, not an implementation.\\n\\nYou can imagine how it\\'s like to see a language model in action, using an interface.\\n\\nI\\'ll start with a simple example, which is a simple one.\\n\\nimport java.io.* ; class ExampleApp extends System.IO. Generic. System { static void main ( String [] args ) { System. out. println ( \"Hello, World!\" ); } }\\n\\nI\\'m not just talking about Java. I\\'m also talking about all the other languages I\\'ve learned in my life, which are so fascinating for me to see how they work.\\n\\nThat\\'s like saying I\\'ll learn all the languages in my life.\\n\\nLet\\'s have a look at some examples:\\n\\n< I\\'m using Java.io.Console, which is a Java.IO.Console class.>\\n\\nI\\'m using Java.IO.Console, which is a java.io.Console class. I\\'m using Java.IO.Console.class, which is a java.io.Console class. I\\'m using Java.IO.Console.class, which is a java.io.Console class. I\\'m using java.io.Console.class, which is a java.io.Console class.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline,set_seed\n",
    "generator = pipeline('text-generation',model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\",max_length=30,num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a6a571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9850)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(768)\n",
    "n = 100\n",
    "\n",
    "for i in range(n):\n",
    "    x += n**-0.5 * torch.randn_like(x)\n",
    "\n",
    "print(x.std())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5d6fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999999999999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1+0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a10f623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7007a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ed7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTconfig:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 65\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 2\n",
    "    n_embd: int = 384\n",
    "    droput = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "610afb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = [10,5,10]\n",
    "x = torch.randn(size)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fe761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,config:GPTconfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.config = config\n",
    "        self.c_attn = nn.Linear(config.n_embd,3*config.n_embd,bias=False)\n",
    "        self.proj = nn.Linear(config.n_embd,config.n_embd,bias=False)\n",
    "        self.dropout = nn.Dropout(config.droput)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size))\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        B,T,C = x.shape\n",
    "        q,k,v = self.c_attn(x).split(self.config.n_embd,dim=-1)\n",
    "\n",
    "        q = q.view(B,T,self.config.n_head,C//self.config.n_head).transpose(1,2) # (10,5,2,5) -> # (10,2,5,5)\n",
    "        k = k.view(B,T,self.config.n_head,C//self.config.n_head).transpose(1,2)\n",
    "        v = v.view(B,T,self.config.n_head,C//self.config.n_head).transpose(1,2)\n",
    "            # (B,n_head,T,T)\n",
    "            # (10,2,5,5) @ (10,2,5,5) --> (10,2,5,5)\n",
    "        att = (q@k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))  # scaling attention product by 1 / sqroot(Dk) to prevent the variance from getting too large\n",
    "        att = att.masked_fill(self.tril[:,:,T,T]==0,float('-inf')) # In general our tril matrix is of size context length, but we never know of what size T are we pulling in now\n",
    "        att  = F.softmax(att,dim=-1)\n",
    "        # att = self.dropout(att)\n",
    "\n",
    "        # (10,2,5,5) @ (10,2,5,5) = (10,2,5,5)\n",
    "        out = att @ v\n",
    "\n",
    "        # out = out.transpose(1,2).contiguous().view(B,T,C)\n",
    "        # out = self.proj(out)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a5f194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTconfig(block_size=10,n_embd=10)\n",
    "attention = CausalSelfAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c89e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 5, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = attention(x)\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "272c596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c47203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Loading weights from pretrained GPT: gpt2\n",
      "tensor([15496,    11,   314,  1101,   257,  3303,  2746,    11,   543,  1724],\n",
      "       device='mps:0')\n",
      "tensor([15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "       device='mps:0')\n",
      "\n",
      "Its taking 0.04 seconds for inference to run when device is set to mps\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import time\n",
    "import tiktoken \n",
    "# device = 'cpu'\n",
    "device = 'mps' if torch.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GPTconfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout:float = 0.0\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config:GPTconfig) -> None:\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,config:GPTconfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.config = config\n",
    "        self.c_attn = nn.Linear(config.n_embd,3*config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd,config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(config.block_size,config.block_size)).view(1,1,config.block_size,config.block_size)) # C,C -> 1,1,C,C (Batched)\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        # intialising our k,q,v\n",
    "        q,k,v = self.c_attn(x).split(self.config.n_embd,dim=-1)\n",
    "        \n",
    "        # We're doing some tensor gymnastics to view our last channel as n_head * head_size, and make the n_head as a Batch Dimension, so that all heads are processed in parallel \n",
    "        # split C into head_size,n_head\n",
    "        q = q.view(B,T,self.config.n_head,C//self.config.n_head).transpose(1,2) # B,n_head,T,head_size\n",
    "        k = k.view(B,T,self.config.n_head,C//self.config.n_head).transpose(1,2) # B,n_head,T,head_size\n",
    "        v = v.view(B,T,self.config.n_head,C//self.config.n_head).transpose(1,2) # B,n_head,T,head_size\n",
    "        \n",
    "    # heart of cauusal-self-attention mechanism 🧐\n",
    "\n",
    "            # (B,T,n_head,head_size) @ (B,T,head_size,n_head) -> (B,T,n_head,n_head)\n",
    "        att = (q@k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))  # scaling attention product by 1 / sqroot(Dk) to prevent the variance from getting too large\n",
    "        # after matmul we get (B,n_heads,num_key_positions,num_query_positions)\n",
    "\n",
    "        # you can comment out this line to get self-attention ⬇️\n",
    "        att = att.masked_fill(self.tril[:,:,:T,:T]==0,float('-inf'))  \n",
    "        att  = F.softmax(att,dim=-1)\n",
    "        att = self.dropout(att)\n",
    "            # (B,T,n_head,n_head) @ (B,T,n_head,head_size) ->   (B,T,n_head,head_size)\n",
    "        y = att @ v \n",
    "            # B,T,n_head,head_size -> B,T,C\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "\n",
    "        # This is our blender, wherein we mix the outputs of all heads before feeding into an MLP\n",
    "        out = self.c_proj(y) \n",
    "\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self,config:GPTconfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Since we do a ln and skip connection before an MLP, its essential for the outputs to be blended beforehand, else it'll be hard to backprop through\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config:GPTconfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size,config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size,config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        self. lm_head = nn.Linear(config.n_embd,config.vocab_size,bias=False)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of lenght {T} when block size is {self.config.block_size}\"\n",
    "        \n",
    "        token_embd = self.transformer.wte(idx) # pyright: ignore[reportCallIssue]\n",
    "\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=idx.device)\n",
    "        position_embd = self.transformer.wpe(pos) # pyright: ignore[reportCallIssue]\n",
    "\n",
    "        x = token_embd + position_embd\n",
    "        x = self.transformer.drop(x) # pyright: ignore[reportCallIssue]\n",
    "        \n",
    "        for block in self.transformer.h: # pyright: ignore[reportGeneralTypeIssues, reportCallIssue]\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x) # pyright: ignore[reportCallIssue]\n",
    "\n",
    "        logits = self.lm_head(x) # (B,T,Vocab_size)\n",
    "\n",
    "        loss = None\n",
    "    \n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,model_type):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(f\"Loading weights from pretrained GPT: {model_type}\")\n",
    "\n",
    "         # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "\n",
    "        config_args['vocab_size'] = 50257\n",
    "        config_args['block_size'] = 1024\n",
    "\n",
    "        config = GPTconfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.tril')]\n",
    "\n",
    "         # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "model = model.to(device)\n",
    "# model = GPT(GPTconfig())\n",
    "model.eval()\n",
    "\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens,dtype=torch.long) #(8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences,1) \n",
    "\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "x = tokens.to(device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42)\n",
    "\n",
    "t0= time.perf_counter()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits,_ = model(x)\n",
    "    logits = logits[:,-1,:]\n",
    "    probs = F.softmax(logits,-1)\n",
    "\n",
    "    topkprobs, topkindices = torch.topk(probs,50)\n",
    "\n",
    "    ix = torch.multinomial(topkprobs,1)\n",
    "\n",
    "    xcol = torch.gather(topkindices,-1,ix)\n",
    "\n",
    "    # x = torch.cat((x,xcol),dim=1)\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "# for i in range(num_return_sequences):\n",
    "#     tokens = x[i,:max_length].tolist()\n",
    "#     decoded = enc.decode(tokens)\n",
    "#     print(\">\",decoded)\n",
    "\n",
    "\n",
    "print('')\n",
    "print(f\"Its taking {(time.perf_counter()-t0):.2f} seconds for inference to run when device is set to {device}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
